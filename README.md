# 人工智能与芯片设计
## 目录
1. [第一章 绪论](#1st)
2. [第二章 监督学习与无监督学习](#2nd)
3. [第三章 python基础（未完成）](#3rd)
4. [第四章 线性回归](#4th)
5. [第五章 多维向量](#5th)
6. [第六章 逻辑回归](#6th)
7. [第七章 人工神经网络](#7th)
8. [第八章 多层神经网络](#8th)
<br>

<a name="1st"></a>
## 第一章 绪论
**什么是大模型**
* 大模型（Large Language Model），全称是大语言模型，它是一种人工智能模型，旨在理解和生成人类语言。

**人工智能的定义**
* 人工智能就是研究如何使计算机去做过去只有人才能做的智能工作
<div style="text-align: right;">
--Patrick Winston, MIT
</div>

**人工智能的研究领域**
1. 机器视觉（Computer Vision）：用机器代替人眼来做测量和判断
2. 图像增强：用于改进图像的质量
3. 自然语言处理（Natural Language Process）：自然语言指自然地随文化演化的语言
4. 强化学习：一个智能体采取行动改变自己的状态，从而获得奖励与环境发生交互的循环过程

**机器学习**
* 研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能
* 它是人工智能核心，是使计算机具有智能的根本途径

**机器学习的开发环境**
* python语言
> python语言的特点：
>1. 脚本语言、解释器
>2. 运行效率低
>3. 开发周期短

**开发框架**
* 框架使构成一类特定软件**可复用设计**的一组相互协作的类

**机器学习的主要框架**
|**公司**|Facebook|Google|
|:--:|:--:|:--:|
|**框架**|Pytorch|Tensorflow|
|||Jax|

**常用库**
|**库**|**功能**|
|:-:|:-:|
|CUDA|并行运算、GPU加速|
|Scikit-Learn|机器学习库|
|Numpy|矩阵运算|
|Pandas|文本处理|
|Matplotlib|绘制图表|
|OpenCV|机器视觉|
|Jupyter Notebook|交互式代码调试和可视化|

<br>

<a name="2nd"></a>
## 第二章 监督学习与无监督学习
**机器学习的定义**
* 机器学习是这样的领域，它赋予计算机学习的能力，这种学习能力**不是**通过**显著式编程**得到的
<div style="text-align: right;">
--Arthur Samuel
</div>

> 让计算机自动总结规律的方法就是非显著式编程

<br>

* 对于某类**任务T**和**性能度量P**，如果一个计算机程序能在T上以P衡量的性能随着**经验E**而自我完善，那么就称这个计算机程序在从经验E学习
<div style="text-align: right;">
--Tom Mitchell，Machine Learning
</div>

> 这种学习的特点是它在T上的被P所衡量的性能，会随着经验E的增加而提高

<br>

**显著式编程的劣势**
1. 对于每个不同的任务和环境，程序员需要每一步都规划好
2. 一旦环境发生改变，之前编写的程序就失效了
> **相反非显著式编写的程序通常可以胜任新的任务**

<br>

**机器学习**
* 机器学习是一门用来**设计算法**的学科，这些算法能够从数据中**构建预测**和**描述模型**
* 机器学习的主要理论基础涉及到概率论、数理统计、数值逼近、最优化理论、计算复杂理论等，核心要素是数据、算法和模型。

**机器学习的两种类型**
* 监督学习：(supervised learning)是指从标注数据中学习预测模型的机器学习问题
>1. 回归
>2. 分类
* 无监督学习：没有标签的机器学习
> 1. 聚类

<br>

<a name="3rd"></a>
## 第三章 python基础
**python可以用来做什么？**
1. 游戏开发
2. 网络爬虫
3. 大数据分析
4. 人工智能

**python的特性**
* python是一种脚本语言
* 它使用解释器而非编译器
* 它的运行效率慢，但优势在于多元化的开发框架和极高的开发效率，非常适合人工智能的科研工作

> **简洁的python**
> * python和c++有很强的相似之处，很多语法甚至可以直接套用
> * 熟悉c++的程序员甚至可以在极短的时间内掌握python的基本语法和编程

**python与c语言的差别**
1. python语句使用**换行**分隔而不需要分号；结尾
2. 使用**缩进**和**冒号**而非大括号{}区分代码的层次
3. 变量**无需声明**，也**不区分**具体**类型**

**代码编辑器**
* VSCode
* PyCharm
* notepad++
* 记事本
* EditPlus

**开发库管理**
* pip
* anaconda

<br>


**第一个程序**
```python
print("Hello world!")
```
python每句结尾不使用分号；
两条代码之间要使用回车换行

<br>

**声明/赋值变量**
```python
n="10086"
print("移动客服电话：")
print(n)
```
**python中的变量在使用之前不需要声明**

<br>

**input()**
* python中，input()函数可以暂停程序，接收键盘输入
```python
# coding: utf-8
a=input("please input:")
print(a)
```

<br>

**注释**
* python的单行注释用#，注释的部分不会被执行
* 块注释使用三引号''' '''，引号中间为注释内容

<br>

**一个使用变量的例子**
```python
# coding:utf-8
a=10
b=0.5
c="Programmer"
print(a)
print(b)
print(c)
```

python还有一些比较重要的变量，布尔值
布尔值只有两种取值：True和False，分别表示逻辑真和假，例如
```python
# coding:utf-8
t=True
f=False
print(t)
print(f)
```
注意这里的`t=True`没有加引号，表面它不是字符串，而是布尔值
```python
# coding:utf-8
t1=True
t2="True"
print(t1)
print(t2)
print(type(t1))
print(type(t2))
```

<br>

**求和**
```python
a=2
b=3
c=a+b
print("c=",c)
```
* python中不需要特别指定变量的类型
* print函数可以打印多个字符串，使用逗号隔开

<br>

```python
c=2
print("c=",c,type(c))
c="厚德博学，求实创新"
print("c=",c,type(c))
c=True
print("c=",c,type(c))
```

<br>

**乘方**
乘方（\*\*）可以用双乘号\*\*计算一个数的n次方
```python
a=3
n=2
c=a**n
print(c)
```

<br>

**整除**
整除(//)用于将一个数除以另一个数，并返回商的整数部分（向下取整）
```python
# coding:utf-8
a,b=7,3
c=a//b
print(c)
```
* 本例中，我们还可以使用`a,b=7,3`的形式在一个语句中同时对两个变量进行幅值，变量名和数字都用逗号隔开即可。如果还要使用三个变量或是多个变量的情况亦是如此

<br>

**逻辑运算**
```python
a=True
b=False
print(a and b)
print(a or b)
print(not a)
print(a ^ b)
```
|python语法|逻辑连接词|
|:-:|:-:|
|and|与|
|or|或|
|not|非|
|^|异或|

**比较运算符**
在python中，比较运算符用于比较两个值之间的关系，如是否相等、是否大于、是否小于登。这些比较运算符返回一个布尔值（True或False）
* 等于（==）
> 用于检查两个值是否相等
```python
#coding:utf-8
a,b=5,5
c=(a==b)
print(c)
```
* 不等于（！=）
> 检查两个值是否不相等
```python
#coding:utf=8
a,b=5,5
c=(a!=b)
print(c)
```
* 其他运算符
> 如：
> * 大于（>）
> * 小于（<）
> * 大于等于（>=）
> * 小于等于（<=）

**循环语句**
* while循环
> python中有时要多次重复执行一个操作，比如
> * 打印10次`Hello world!`
> 
> 固然你可以把`print('Hello world!')`这个命令复制粘贴10行，但如果让你打印全校上万学生的姓名，你又要复制多少行呢？
> 
> 一个有经验的程序员会选择更效率的方法

while循环是一种基本的控制流语句，它允许程序在满足特定条件时重复执行一段代码

当条件为True时，while循环内的代码块会不断执行，直到条件变为False为止

例如：
```python
#coding:utf=8
while(True)
    print(1)
```

**语法这部分比较简单后续有时间再写**
**语法这部分比较简单后续有时间再写**
**语法这部分比较简单后续有时间再写**

<br>

<a name="4th"></a>
## 第四章 线性回归
**常见的两种监督学习**
* 回归
> 拟合一条曲线

* 分类
> 结果只有少量表标签，图上看时一些离散的无规则点

<br>

**线性回归**
* **线性回归**是利用数理统计种的**回归分析**，确定两种或两种以上变量相互依赖的定量关系
* 线性回归也是最广泛使用和基本的机器学习模型

<br>

**数据集**
数据集（Dataset）是一个数据的集合，通常以表格的形式出现，一般分为三类
* 训练集（Training Set）
> 训练集是机器学习中用来训练模型的数据集。在这个数据集上，算法会学习如何根据输入的特征（或属性）来预测输出（或标签）。训练过程会不断调整模型的参数，以最小化预测值与实际值之间的差异，即优化模型的性能。训练集通常包含大量的数据样本，以便模能够学习搭配足够的特征和模式，从而对新的、未见过的数据做出准确的预测。
* 验证集（Validation Set）
> 验证集是在训练过程中用于调整模型超参数的数据集。超参数是那些在开始学习过程之前没设置的参数，而不是通过训练过程学习的参数（如学习率、神经网络层数、每层的节点数等）。验证集的主要目的是帮助我们在训练过程中选择最佳的模型结构和参数设置。通过比较不同超参数配置下模型在验证集上的性能，我们可以选择出泛化能力最好的模型配置。需要注意的是，验证集不参与模型的训练过程，以避免模型对验证集产生过拟合。
* 测试集（Test Set）
> 测试集是用于评估最终模型性能的数据集。在模型训练完成后，我们使用测试集来评估模型的泛化能力，即模型对未见过数据的预测能力。测试集应该与训练集和验证集保持独立，以确保评估结果的客观性和准确性。通过比较模型在测试集上的预测结果与实际结果，我们可以了解模型的实际性能，并据此对模型进行改进或调整。

<br>

**通用的算法流程**
训练集（特征变量/目标变量）->学习算法->函数f
x（特征）->函数f（模型）->y（预测）

<br>

**如何表示函数f**
* $f_{w,b}=w*x+b$
* w,b：参数（parameters）
* 权重（weight），偏置（bias）

<br>

**损失函数**
* 损失函数（Loss Function）是机器学习中用于评估模型**预测值**与**真实值**之间差异程度的函数
* 它的主要目的是指导模型的训练过程，通过最小化损失函数的值来优化模型的参数，从而使模型的预测结果更加接近真实值
> $(x^{(i)},y^{(i)})$与$(x^{(i)},\hat{y}^{(i)})$之间的距离越小，说明模型对第$i$个点的预测越准确
> $(D^{(i)})^2=(x^{(i)}-x^{(i)})^2+(\hat{y}^{(i)}-y^{(i)})^2=(\hat{y}^{(i)}-y^{(i)})^2$
> $(\hat{y}^{(i)}-y^{(i)})^2$越小，预测越准确
> 对于训练集的所有样本，对每一个$i$都要做出尽量最好的预测，就是要让$(D^{(i)})^2$的平均值最小
* 例如：均方根误差（Mean Squared Error，MSE）
$$
\frac{1}{2m}\Sigma^{m}_{i=1}(\hat{y}^{(i)}-y^{(i)})^2
$$
$$
J(w,b)=\frac{1}{2m}\Sigma^{m}_{i=1}(\hat{y}^{(i)}-y^{(i)})^2
$$

<br>

**梯度下降算法**
计算下次迭代参数应该调整的位置，用于快速找到局部最小值
$$
J=f(w,b)
dJ=\frac{\partial{f}}{\partial{w}}dw+\frac{\partial{f}}{\partial{b}}db
$$
所以此时得到了损失函数的梯度函数，用其对原始值进行迭代
$$
w=w-\alpha{\frac{\partial}{\partial{w}}J(w,b)}
$$
$$
b=b-\alpha \frac{\partial}{\partial{b}} J(w,b)
$$
$$
\alpha : 学习率（\alpha > 0 ,\alpha \in [10^{-4},10^{-3}]）
$$
* 梯度下降算法目前仍然使深度学习领域使用的主流优化算法之一
* 还有一些其它算法，例如
  - 随机梯度下降（SGD，Stochastic Gradient Descent）
  - Adam
  - AdamW等 
> 注意：当学习率过小时，收敛缓慢，当学习率过大时可能会在收敛值附近波动

<br>

**线性回归的梯度下降**
线性回归模型：
$$
f_{w,b}(x)=wx+b
$$
损失函数
$$
J(w,b)=\frac{1}{2m}\Sigma_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2
$$
梯度下降算法
$$
w=w-\alpha \frac{\partial}{\partial w}J(w,b) \to w=w-\alpha \frac{1}{m}\Sigma_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}
$$
$$
b=b-\alpha \frac{\partial}{\partial b}J(w,b) \to b=b-\alpha \frac{1}{m}\Sigma_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})
$$

<br>

<a name="5th"></a>
## 第五章 多维向量
**一维与多维**
一维特征线性回归模型
$$
f_{w,b}(x)=wx+b
$$
多维线性回归模型
$$
f_{w,b}(x)=w_1x_1+w_2x_2+...+w_nx_n+b
$$
向量形式
$$
f_{\vec{w},b}=w_1x_1+w_2x_2+...+w_nx_n+b
$$
$$
\vec{w}=[w_1,w_2,w_3,...,w_n]
$$
$$
b是一个常数
$$
$$
\vec{x}=[x_1,x_2,x_3,...,x_n]
$$
$$
f_{\vec{w},b}=\vec{w}\cdot{\vec{x}}+b=w_1x_1+w_2x_2+...+w_nx_n+b=w^Tx+b
$$

**代码**
```python
w=np.array([1.0,2.5,-3.3])
b=4
x=np.array([10,20,30])
f=np.dot(w,x)+b
```

**向量式的梯度下降**
$$
w_j=w_j-\alpha \frac{\partial}{\partial w}J(\vec{w},b)
$$

$$
b=b-\alpha \frac{\partial}{\partial b}J(\vec{w},b)
$$

<br>

**注意点**
* 学习曲线应该是逐渐下降并最终收敛的若不是，则考虑学习率是否过大
* 当几个参数数量级相差过大时，可以先对数据进行规范化，以去除数量级的影响 

## 第六章 逻辑回归
<a name="6th"></a>
**什么是逻辑回归**
* 逻辑回归是通过逻辑函数将线性回归模型的输出转换为概率值，这个概率值表示了样本属于某一类别的可能性
* 通过概率的大小，我们可以判断样本所属的类别
* 逻辑回归可用于求解一些二分类问题

**sigmoid函数**
逻辑回归所用到的函数
$$
g(x)=\frac{1}{1+e^{-x}}
$$

**逻辑回归函数**
$$
g(z)=\frac{1}{1+e^{-z}} \to f_{\vec{w},b}(x)=\frac{1}{1+e^{-(\vec{w}\cdot \vec{x}+b)}}
$$

![sigmoid函数](https://i-blog.csdnimg.cn/blog_migrate/154c39796bb0a440de7b6a36835e7b6c.png)

当$x\ge 0 （即g \ge 0.5）$时，认为$\hat{y}=1$，否则认为$\hat{y}=0$.
也即：
$$
\vec{w}\cdot \vec{x}+b \ge 0 \  时:\hat{y}=1
$$
$$
\vec{w}\cdot \vec{x}+b \lt 0 \  时:\hat{y}=0
$$

<br>

**决策边界**
决策边界是在机器许欸小中用来区分不同类别样本的分界
例如对于线性决策边界
$$
f_{\vec{x},b}(\vec{x})=g(z)=g(w_1x_1+w_2x_2+b)
$$
当$w_1=1, w_2=1, b=3$时，决策边界为：
$$
z=\vec{w} \cdot \vec{x}+b=0
$$
$$
z=x_1+x_2-3=3
$$
$$
x_1+x_2=3
$$
![线性决策边界](https://i-blog.csdnimg.cn/direct/61aa28f3b43f4cff9035805d1400e790.jpeg)

对于非线性决策边界
$$
f_{\vec{x},b}(\vec{x})=g(z)=g(w_1x_1^2+w_2x_2^2+b)
$$
当$w_1=1, w_2=1, b=-1$时，决策边界为：
$$
z=x_1^2+x_2^2-1=0
$$
$$
x_1^2+x_2^2=1
$$
![非线性决策边界](https://i-blog.csdnimg.cn/direct/cf2eda74186d4f18b8886577298ad0e6.jpeg)

<br>

**逻辑回归损失函数**
负对数似然损失函数

$$ f(x)=\left\{
\begin{aligned}
-log(f_{\vec{w},b}(\vec{x}^{(i)})) \quad\quad  y^{(i)}=1\\
-log(1-f_{\vec{w},b}(\vec{x}^{(i)})) \quad\quad  y^{(i)}=0
\end{aligned}
\right.
$$

简化形式

$$
L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})=-y^{(i)}log(f_{\vec{w},b}(\vec{x}))-(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))
$$

所以模型的损失函数为：
$$
J(\vec{w},b)=-\frac{1}{m} \Sigma^{m}_{i=1}[y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)}))+(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))]
$$

<br>

**梯度下降法更新参数**
$$
w_j=w_j-\alpha [\frac{1}{m}\Sigma^{m}_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)}-y^{(i)})x_j^{(i)})]
$$

$$
b=b-\alpha [\frac{1}{m}\Sigma^{m}_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)}-y^{(i)})x_j^{(i)})]
$$


## 第七章 人工神经网络
<a name="7th"></a>
人工神经网络的基本思想是**仿生学**，即对人脑的**神经元**运作机制进行模拟

<br>

**人工智能的两种学派**
* 仿生学派
* 数理学派

<br>

**仿生学派**
仿生学派认为人工智能模拟的是人类大脑对于世界的认识，因此研究大脑的认知机制，总结大脑处理信息的方式，是实现人工智能的先决条件。
仿生学派认为计算机算法只有深入的模拟大脑的认知机制和信息处理方式才能最终实现人工智能。

**数理学派**
数里学派却认为在宣布在及可预知的未来，人类无法完全了解人脑的认知机制。
计算机与人脑具有截然不同的物理属性和体系结构，因此片面强调计算机对人脑的模仿既不可能也不必须。
人工智能的研究应该立足于现有的计算机的物理属性和体系结构，用数学和逻辑推理的方法，从现有的计算机结构中获得确定的知识，而不是一昧的强调对人脑的模仿，就好比飞机的飞行并不像鸟却能比鸟飞得更快。

<br>

**MP模型**
1943年，心理学家w.s.McCulloch和数理逻辑学家W.Pitts基于神经元的生理特征，建立了单个神经元的数学模型（MP模型）
**MP模型**是将外部刺激模拟为一串数字$x_1,x_2,...,x_m$的**输入**，将每个树突对**输入**的刺激加工过程模拟为以某个**权重**对**输入**进行加权，将细胞核对输入的处理模拟为一个带有**偏执**的**求和**过程，最后输出的是用几乎哦函数对求和的结果进行**非线性变换**而得出。
![神经元的数学模型示意图](https://s2.51cto.com/images/blog/202107/13/2954f70096cdce9c4f5e4e7e37b43971.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

<br>

**感知机模型**
1957年，Frank Rosenblatt从纯数学的角度重新考察这一模型，指出：
* 能够通过学习算法从一些输入输出对$X,y$中获得权重$W$和$b$

问题：给定一些输入输出对$X,y$,其中$y=\pm 1$，求一个函数，使$f(X)=y$
**感知机算法**：设定$f(X)=sign(W^TX+b)$，从一堆输入输出中自动学习，获得$W$和$b$.

<br>

**线性可分与线性不可分**
![线性可分和线性不可分](https://i-blog.csdnimg.cn/blog_migrate/b9499f27b7f1fa7b4391781ed5ca321e.png)

<br>

**线性可分的数学定义**
一个训练集$\{(X_i,y_i),...,(X_N,y_N)\}$，在$i=1 \sim N$线性可分，是指存在$(\omega_1,\omega_2,b)$，使得对$i=1~N$，有：
1. 若$y_i=+1$，则$\omega_1x_{i1}+\omega_2x_{i2}+b>0$
2. 若$y_i=-1$，则$\omega_1x_{i1}+\omega_2x_{i2}+b<0$

**向量形式**
假设：
$$
    x_i=\begin{bmatrix}
x_{i2}\\
x_{i2} 
\end{bmatrix}^T
$$

$$
    \omega=\begin{bmatrix}
\omega_1\\
\omega_2
\end{bmatrix}^T
$$
1. 若$y_i=+1$，则$\omega^Tx_i+b>0$
2. 若$y_i=-1$，则$\omega^Tx_i+b<0$

**线性可分定义的最简化形式**
1. 若$y_i=+1$，则$\omega^Tx_i+b>0$
2. 若$y_i=-1$，则$\omega^Tx_i+b<0$
如果：$y_i=+1$或$-1$
一个训练集$\{(X_i,y_i)\}$，在$i=1 \sim N$线性可分，是指存在$(\omega,b)$，使得对$i=1 \sim N$，有：
$$
y_i(\omega^TX_i+b)>0
$$

<br>

**感知机算法（Perceptron Algorithm）**
1. 随机选择$W$和$b$
2. 取一个训练样本$(X,y)$
* 若$W^TX+b>0$且$y=-1$，则
    $w=w-x,b=b-1$
* 若$W^TX+b<0$且$y=+1$，则
    $w=w+x,b=b+1$
3. 再取另一个$X,y$，回到2
**终止条件**：直到所有输入输出对都不满足2中两条之一，退出循环。

**感知机算法的优点与缺陷**
感知机算法虽然对比其他模型并没有优势，且不能解决线性不可分问题，但其启发了神经网络的发展。

<br>

**神经网络**
* 在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个**非线性的**函数关系，这个函数称为激活函数。
* 如果层与层之间不加非线性函数，多层神经网络会退化为单神经元感知机。
* 证明：（以最简单两层神经网络为例）
对于网络
$$
a_1=\omega_{11}x_1+\omega_{12}x_2+b_1 \quad 第一个神经元
$$

$$
a_2=\omega_{21}x_1+\omega_{22}x_2+b_2 \quad 第二个神经元
$$

$$
Z_1=\varphi(a_1)    \quad    非线性函数
$$

$$
Z_2=\varphi(a_2)    \quad    非线性函数
$$

$$
y=\omega_1z_1+\omega_2z_2+b_3    \quad    第三个神经元
$$
有
$$
y=\omega_1(\omega_{11}x_1+\omega_{12}x_2+b_1)+\omega_2(\omega_{21}x_1+\omega_{22}x_2+b_2)+b_3
$$

$$
y=(\omega_1\omega_{11}+\omega_2\omega_{21})x_1+(\omega_1\omega_{12}+\omega_2\omega_{22})x_2+\omega_1b_1+\omega_2b_2+b_3
$$
设$W_1,W_2,B$为三个常数
$$
y=W_1x_1+W_2x_2+B
$$
如果层与层之间不加非线性函数，多层神经网络会退化为单神经元感知机

**部分激活函数**
![激活函数](https://pic2.zhimg.com/v2-2762505c22da213c53765fd5a2ccf8c5_1440w.jpg)
**定理**：当$\varphi(x)$为阶跃函数时，三层神经网络可模拟任意非线性函数

<br>

## 第八章 多层神经网络
<a name="8th"></a>


**反向传播算法**
反向传播算法的梯度求解利用了链式求导法则
反向传播的流程：
1. 前向传播
2. 计算损失
3. 反向传播
4. 更新参数
5. 反复迭代
