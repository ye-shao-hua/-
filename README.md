
# 人工智能与芯片设计
## 目录
1. [第一章 绪论](#1st)
2. [第二章 监督学习与无监督学习](#2nd)
3. [第三章 python基础（未完成）](#3rd)
4. [第四章 线性回归](#4th)
5. [第五章 多维向量](#5th)
6. [第六章 逻辑回归](#6th)
7. [第七章 人工神经网络](#7th)
8. [第八章 多层神经网络](#8th)
9. [第九章 卷积神经网络](#9th)
10. [第十章 循环神经网络](#10th)
11. [第十一章 自然语言处理](#11st)


<a name="1st"></a>
## 第一章 绪论
**什么是大模型**
* 大模型（Large Language Model），全称是大语言模型，它是一种人工智能模型，旨在理解和生成人类语言。

<br>

**人工智能的定义**
* 人工智能就是研究如何使计算机去做过去只有人才能做的智能工作
<div style="text-align: right;">
--Patrick Winston, MIT
</div>

<br>

**人工智能的研究领域**
1. 机器视觉（Computer Vision）：用机器代替人眼来做测量和判断
2. 图像增强：用于改进图像的质量
3. 自然语言处理（Natural Language Process）：自然语言指自然地随文化演化的语言
4. 强化学习：一个智能体采取行动改变自己的状态，从而获得奖励与环境发生交互的循环过程

<br>

**机器学习**
* 研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能
* 它是人工智能核心，是使计算机具有智能的根本途径

<br>

**机器学习的开发环境**
* python语言
> python语言的特点：
>1. 脚本语言、解释器
>2. 运行效率低
>3. 开发周期短

<br>

**开发框架**
* 框架使构成一类特定软件**可复用设计**的一组相互协作的类

<br>

**机器学习的主要框架**
|**公司**|Facebook|Google|
|:--:|:--:|:--:|
|**框架**|Pytorch|Tensorflow|
|||Jax|

<br>

**常用库**
|**库**|**功能**|
|:-:|:-:|
|CUDA|并行运算、GPU加速|
|Scikit-Learn|机器学习库|
|Numpy|矩阵运算|
|Pandas|文本处理|
|Matplotlib|绘制图表|
|OpenCV|机器视觉|
|Jupyter Notebook|交互式代码调试和可视化|

<br>

<a name="2nd"></a>
## 第二章 监督学习与无监督学习
**机器学习的定义**
* 机器学习是这样的领域，它赋予计算机学习的能力，这种学习能力**不是**通过**显著式编程**得到的
<div style="text-align: right;">
--Arthur Samuel
</div>

> 让计算机自动总结规律的方法就是非显著式编程

<br>

* 对于某类**任务T**和**性能度量P**，如果一个计算机程序能在T上以P衡量的性能随着**经验E**而自我完善，那么就称这个计算机程序在从经验E学习
<div style="text-align: right;">
--Tom Mitchell，Machine Learning
</div>

> 这种学习的特点是它在T上的被P所衡量的性能，会随着经验E的增加而提高

<br>

**显著式编程的劣势**
1. 对于每个不同的任务和环境，程序员需要每一步都规划好
2. 一旦环境发生改变，之前编写的程序就失效了
> **相反非显著式编写的程序通常可以胜任新的任务**

<br>

**机器学习**
* 机器学习是一门用来**设计算法**的学科，这些算法能够从数据中**构建预测**和**描述模型**
* 机器学习的主要理论基础涉及到概率论、数理统计、数值逼近、最优化理论、计算复杂理论等，核心要素是数据、算法和模型。

**机器学习的两种类型**
* 监督学习：(supervised learning)是指从标注数据中学习预测模型的机器学习问题
>1. 回归
>2. 分类
* 无监督学习：没有标签的机器学习
> 1. 聚类

<br>

<a name="3rd"></a>
## 第三章 python基础
**python可以用来做什么？**
1. 游戏开发
2. 网络爬虫
3. 大数据分析
4. 人工智能

**python的特性**
* python是一种脚本语言
* 它使用解释器而非编译器
* 它的运行效率慢，但优势在于多元化的开发框架和极高的开发效率，非常适合人工智能的科研工作

> **简洁的python**
> * python和c++有很强的相似之处，很多语法甚至可以直接套用
> * 熟悉c++的程序员甚至可以在极短的时间内掌握python的基本语法和编程

**python与c语言的差别**
1. python语句使用**换行**分隔而不需要分号；结尾
2. 使用**缩进**和**冒号**而非大括号{}区分代码的层次
3. 变量**无需声明**，也**不区分**具体**类型**

**代码编辑器**
* VSCode
* PyCharm
* notepad++
* 记事本
* EditPlus

**开发库管理**
* pip
* anaconda

<br>


**第一个程序**
```python=
print("Hello world!")
```
python每句结尾不使用分号；
两条代码之间要使用回车换行

<br>

**声明/赋值变量**
```python=
n="10086"
print("移动客服电话：")
print(n)
```
**python中的变量在使用之前不需要声明**

<br>

**input()**
* python中，input()函数可以暂停程序，接收键盘输入
```python=
# coding: utf-8
a=input("please input:")
print(a)
```

<br>

**注释**
* python的单行注释用#，注释的部分不会被执行
* 块注释使用三引号''' '''，引号中间为注释内容

<br>

**一个使用变量的例子**
```python=
# coding:utf-8
a=10
b=0.5
c="Programmer"
print(a)
print(b)
print(c)
```

python还有一些比较重要的变量，布尔值
布尔值只有两种取值：True和False，分别表示逻辑真和假，例如
```python=
# coding:utf-8
t=True
f=False
print(t)
print(f)
```
注意这里的`t=True`没有加引号，表面它不是字符串，而是布尔值
```python=
# coding:utf-8
t1=True
t2="True"
print(t1)
print(t2)
print(type(t1))
print(type(t2))
```

<br>

**求和**
```python=
a=2
b=3
c=a+b
print("c=",c)
```
* python中不需要特别指定变量的类型
* print函数可以打印多个字符串，使用逗号隔开

<br>

```python=
c=2
print("c=",c,type(c))
c="厚德博学，求实创新"
print("c=",c,type(c))
c=True
print("c=",c,type(c))
```

<br>

**乘方**

乘方（\*\*）可以用双乘号\*\*计算一个数的n次方
```python=
a=3
n=2
c=a**n
print(c)
```

<br>

**整除**

整除(//)用于将一个数除以另一个数，并返回商的整数部分（向下取整）
```python=
# coding:utf-8
a,b=7,3
c=a//b
print(c)
```
* 本例中，我们还可以使用`a,b=7,3`的形式在一个语句中同时对两个变量进行幅值，变量名和数字都用逗号隔开即可。如果还要使用三个变量或是多个变量的情况亦是如此

<br>

**逻辑运算**
```python=
a=True
b=False
print(a and b)
print(a or b)
print(not a)
print(a ^ b)
```
|python语法|逻辑连接词|
|:-:|:-:|
|and|与|
|or|或|
|not|非|
|^|异或|

**比较运算符**

在python中，比较运算符用于比较两个值之间的关系，如是否相等、是否大于、是否小于登。这些比较运算符返回一个布尔值（True或False）
* 等于（==）
> 用于检查两个值是否相等
```python=
#coding:utf-8
a,b=5,5
c=(a==b)
print(c)
```
* 不等于（！=）
> 检查两个值是否不相等
```python=
#coding:utf=8
a,b=5,5
c=(a!=b)
print(c)
```
* 其他运算符
> 如：
> * 大于（>）
> * 小于（<）
> * 大于等于（>=）
> * 小于等于（<=）

**循环语句**
* while循环
> python中有时要多次重复执行一个操作，比如
> * 打印10次`Hello world!`
> 
> 固然你可以把`print('Hello world!')`这个命令复制粘贴10行，但如果让你打印全校上万学生的姓名，你又要复制多少行呢？
> 
> 一个有经验的程序员会选择更效率的方法

while循环是一种基本的控制流语句，它允许程序在满足特定条件时重复执行一段代码

当条件为True时，while循环内的代码块会不断执行，直到条件变为False为止

例如：
```python=
#coding:utf=8
while(True)
    print(1)
```

**语法这部分比较简单后续有时间再写**
**语法这部分比较简单后续有时间再写**
**语法这部分比较简单后续有时间再写**

<br>

<a name="4th"></a>
## 第四章 线性回归
**常见的两种监督学习**
* 回归
> 拟合一条曲线

* 分类
> 结果只有少量表标签，图上看时一些离散的无规则点

<br>

**线性回归**
* **线性回归**是利用数理统计种的**回归分析**，确定两种或两种以上变量相互依赖的定量关系
* 线性回归也是最广泛使用和基本的机器学习模型

<br>

**数据集**

数据集（Dataset）是一个数据的集合，通常以表格的形式出现，一般分为三类
* 训练集（Training Set）
> 训练集是机器学习中用来训练模型的数据集。在这个数据集上，算法会学习如何根据输入的特征（或属性）来预测输出（或标签）。训练过程会不断调整模型的参数，以最小化预测值与实际值之间的差异，即优化模型的性能。训练集通常包含大量的数据样本，以便模能够学习搭配足够的特征和模式，从而对新的、未见过的数据做出准确的预测。
* 验证集（Validation Set）
> 验证集是在训练过程中用于调整模型超参数的数据集。超参数是那些在开始学习过程之前没设置的参数，而不是通过训练过程学习的参数（如学习率、神经网络层数、每层的节点数等）。验证集的主要目的是帮助我们在训练过程中选择最佳的模型结构和参数设置。通过比较不同超参数配置下模型在验证集上的性能，我们可以选择出泛化能力最好的模型配置。需要注意的是，验证集不参与模型的训练过程，以避免模型对验证集产生过拟合。
* 测试集（Test Set）
> 测试集是用于评估最终模型性能的数据集。在模型训练完成后，我们使用测试集来评估模型的泛化能力，即模型对未见过数据的预测能力。测试集应该与训练集和验证集保持独立，以确保评估结果的客观性和准确性。通过比较模型在测试集上的预测结果与实际结果，我们可以了解模型的实际性能，并据此对模型进行改进或调整。

<br>

**通用的算法流程**

训练集（特征变量/目标变量）->学习算法->函数f
x（特征）->函数f（模型）->y（预测）

<br>

**如何表示函数f**
* $f_{w,b}=w*x+b$
* $w,b$：参数（parameters）
* 权重（weight），偏置（bias）

<br>

**损失函数**
* 损失函数（Loss Function）是机器学习中用于评估模型**预测值**与**真实值**之间差异程度的函数
* 它的主要目的是指导模型的训练过程，通过最小化损失函数的值来优化模型的参数，从而使模型的预测结果更加接近真实值
> $(x^{(i)},y^{(i)})$ 与 $(x^{(i)},\hat{y}^{(i)})$ 之间的距离越小，说明模型对第 $i$ 个点的预测越准确
> $(D^{(i)})^2=(x^{(i)}-x^{(i)})^2+(\hat{y}^{(i)}-y^{(i)})^2=(\hat{y}^{(i)}-y^{(i)})^2$
> $(\hat{y}^{(i)}-y^{(i)})^2$ 越小，预测越准确
> 对于训练集的所有样本，对每一个 $i$ 都要做出尽量最好的预测，就是要让 $(D^{(i)})^2$ 的平均值最小
* 例如：均方根误差（Mean Squared Error，MSE）

$$
\frac{1}{2m}\Sigma^{m}_{i=1}(\hat{y}^{(i)}-y^{(i)})^2
$$

$$
J(w,b)=\frac{1}{2m}\Sigma^{m}_{i=1}(\hat{y}^{(i)}-y^{(i)})^2
$$

<br>

**梯度下降算法**

计算下次迭代参数应该调整的位置，用于快速找到局部最小值

$$
J=f(w,b)
$$

$$
dJ=\frac{\partial{f}}{\partial{w}}dw+\frac{\partial{f}}{\partial{b}}db
$$

所以此时得到了损失函数的梯度函数，用其对原始值进行迭代

$$
w=w-\alpha{\frac{\partial}{\partial{w}}J(w,b)}
$$

$$
b=b-\alpha \frac{\partial}{\partial{b}} J(w,b)
$$

$$
\alpha : 学习率（\alpha > 0 ,\alpha \in [10^{-4},10^{-3}]）
$$

* 梯度下降算法目前仍然使深度学习领域使用的主流优化算法之一
* 还有一些其它算法，例如
  - 随机梯度下降（SGD，Stochastic Gradient Descent）
  - Adam
  - AdamW等 
> 注意：当学习率过小时，收敛缓慢，当学习率过大时可能会在收敛值附近波动

<br>

**线性回归的梯度下降**

线性回归模型：

$$
f_{w,b}(x)=wx+b
$$

损失函数

$$
J(w,b)=\frac{1}{2m}\Sigma_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2
$$

梯度下降算法

$$
w=w-\alpha \frac{\partial}{\partial w}J(w,b) \to w=w-\alpha \frac{1}{m}\Sigma_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}
$$

$$
b=b-\alpha \frac{\partial}{\partial b}J(w,b) \to b=b-\alpha \frac{1}{m}\Sigma_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})
$$

<br>

<a name="5th"></a>
## 第五章 多维向量
**一维与多维**

一维特征线性回归模型

$$
f_{w,b}(x)=wx+b
$$

多维线性回归模型

$$
f_{w,b}(x)=w_1x_1+w_2x_2+...+w_nx_n+b
$$

向量形式

$$
f_{\vec{w},b}=w_1x_1+w_2x_2+...+w_nx_n+b
$$

$$
\vec{w}=[w_1,w_2,w_3,...,w_n]
$$

$$
b是一个常数
$$

$$
\vec{x}=[x_1,x_2,x_3,...,x_n]
$$

$$
f_{\vec{w},b}=\vec{w}\cdot{\vec{x}}+b=w_1x_1+w_2x_2+...+w_nx_n+b=w^Tx+b
$$

**代码**
```python=
w=np.array([1.0,2.5,-3.3])
b=4
x=np.array([10,20,30])
f=np.dot(w,x)+b
```

**向量式的梯度下降**

$$
w_j=w_j-\alpha \frac{\partial}{\partial w}J(\vec{w},b)
$$

$$
b=b-\alpha \frac{\partial}{\partial b}J(\vec{w},b)
$$

<br>

**注意点**
* 学习曲线应该是逐渐下降并最终收敛的若不是，则考虑学习率是否过大
* 当几个参数数量级相差过大时，可以先对数据进行规范化，以去除数量级的影响 

## 第六章 逻辑回归

<a name="6th"></a>
**什么是逻辑回归**
* 逻辑回归是通过逻辑函数将线性回归模型的输出转换为概率值，这个概率值表示了样本属于某一类别的可能性
* 通过概率的大小，我们可以判断样本所属的类别
* 逻辑回归可用于求解一些二分类问题

**sigmoid函数**

逻辑回归所用到的函数

$$
g(x)=\frac{1}{1+e^{-x}}
$$

**逻辑回归函数**

$$
g(z)=\frac{1}{1+e^{-z}} \to f_{\vec{w},b}(x)=\frac{1}{1+e^{-(\vec{w}\cdot \vec{x}+b)}}
$$

![sigmoid函数](https://i-blog.csdnimg.cn/blog_migrate/154c39796bb0a440de7b6a36835e7b6c.png)

当 $x\ge 0 （即g \ge 0.5）$ 时，认为 $\hat{y}=1$ ，否则认为 $\hat{y}=0$ 。
也即：

$$
\vec{w}\cdot \vec{x}+b \ge 0 \  时:\hat{y}=1
$$

$$
\vec{w}\cdot \vec{x}+b \lt 0 \  时:\hat{y}=0
$$

<br>

**决策边界**

决策边界是在机器许欸小中用来区分不同类别样本的分界
例如对于线性决策边界

$$
f_{\vec{x},b}(\vec{x})=g(z)=g(w_1x_1+w_2x_2+b)
$$

当 $w_1=1, w_2=1, b=3$时，决策边界为：

$$
z=\vec{w} \cdot \vec{x}+b=0
$$

$$
z=x_1+x_2-3=3
$$

$$
x_1+x_2=3
$$

![线性决策边界](https://i-blog.csdnimg.cn/direct/61aa28f3b43f4cff9035805d1400e790.jpeg)

对于非线性决策边界

$$
f_{\vec{x},b}(\vec{x})=g(z)=g(w_1x_1^2+w_2x_2^2+b)
$$

当 $w_1=1, w_2=1, b=-1$时，决策边界为：

$$
z=x_1^2+x_2^2-1=0
$$

$$
x_1^2+x_2^2=1
$$

![非线性决策边界](https://i-blog.csdnimg.cn/direct/cf2eda74186d4f18b8886577298ad0e6.jpeg)

<br>

**逻辑回归损失函数**

负对数似然损失函数

![负对数似然损失函数](https://i-blog.csdnimg.cn/blog_migrate/60986885615a649fca2da9492cf4fcbc.png)

$$ 
f(x)=
\begin{cases}
-log(f_{\vec{w},b}(\vec{x}^{(i)})) \quad\quad  y^{(i)}=1\\
-log(1-f_{\vec{w},b}(\vec{x}^{(i)})) \quad\quad  y^{(i)}=0.
\end{cases}
$$

简化形式

$$
L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})=-y^{(i)}log(f_{\vec{w},b}(\vec{x}))-(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))
$$

所以模型的损失函数为：

$$
J(\vec{w},b)=-\frac{1}{m} \Sigma^{m}\_{i=1}[y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)}))+(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))]
$$

<br>

**梯度下降法更新参数**

$$
w_j=w_j-\alpha [\frac{1}{m}\Sigma^{m}\_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)}-y^{(i)})x_j^{(i)})]
$$

$$
b=b-\alpha [\frac{1}{m}\Sigma^{m}\_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)}-y^{(i)})x_j^{(i)})]
$$


## 第七章 人工神经网络

<a name="7th"></a>
人工神经网络的基本思想是**仿生学**，即对人脑的**神经元**运作机制进行模拟

<br>

**人工智能的两种学派**
* 仿生学派
* 数理学派

<br>

**仿生学派**

仿生学派认为人工智能模拟的是人类大脑对于世界的认识，因此研究大脑的认知机制，总结大脑处理信息的方式，是实现人工智能的先决条件。
仿生学派认为计算机算法只有深入的模拟大脑的认知机制和信息处理方式才能最终实现人工智能。

**数理学派**

数里学派却认为在宣布在及可预知的未来，人类无法完全了解人脑的认知机制。
计算机与人脑具有截然不同的物理属性和体系结构，因此片面强调计算机对人脑的模仿既不可能也不必须。
人工智能的研究应该立足于现有的计算机的物理属性和体系结构，用数学和逻辑推理的方法，从现有的计算机结构中获得确定的知识，而不是一昧的强调对人脑的模仿，就好比飞机的飞行并不像鸟却能比鸟飞得更快。

<br>

**MP模型**

1943年，心理学家w.s.McCulloch和数理逻辑学家W.Pitts基于神经元的生理特征，建立了单个神经元的数学模型（MP模型）
**MP模型**是将外部刺激模拟为一串数字$x_1,x_2,...,x_m$的**输入**，将每个树突对**输入**的刺激加工过程模拟为以某个**权重**对**输入**进行加权，将细胞核对输入的处理模拟为一个带有**偏执**的**求和**过程，最后输出的是用几乎哦函数对求和的结果进行**非线性变换**而得出。

![神经元的数学模型示意图](https://pic1.zhimg.com/v2-3b6f17c98ce02ae3aca472b58a828d46_r.jpg)

<br>

**感知机模型**

1957年，Frank Rosenblatt从纯数学的角度重新考察这一模型，指出：
* 能够通过学习算法从一些输入输出对 $X,y$中获得权重 $W$和 $b$

问题：给定一些输入输出对 $X,y$,其中 $y=\pm 1$，求一个函数，使 $f(X)=y$
**感知机算法**：设定 $f(X)=sign(W^TX+b)$，从一堆输入输出中自动学习，获得 $W$和 $b$.

<br>

**线性可分与线性不可分**

![线性可分和线性不可分](https://i-blog.csdnimg.cn/blog_migrate/b9499f27b7f1fa7b4391781ed5ca321e.png)

<br>

**线性可分的数学定义**

一个训练集 $\{(X_i,y_i),...,(X_N,y_N)\}$ ，在 $i=1 \sim N$ 线性可分，是指存在 $(\omega_1,\omega_2,b)$，使得对 $i=1~N$，有：
1. 若 $y_i=+1$ ，则 $\omega_1x_{i1}+\omega_2x_{i2}+b>0$ 
2. 若 $y_i=-1$ ，则 $\omega_1x_{i1}+\omega_2x_{i2}+b<0$ 

**向量形式**

假设：

$$
    x_i=\begin{bmatrix}
x_{i2}\\
x_{i2} 
\end{bmatrix}^T
$$

$$
    \omega=\begin{bmatrix}
\omega_1\\
\omega_2
\end{bmatrix}^T
$$

1. 若 $y_i=+1$ ，则 $\omega^Tx_i+b>0$ 
2. 若 $y_i=-1$ ，则 $\omega^Tx_i+b<0$ 

**线性可分定义的最简化形式**
1. 若 $y_i=+1$ ，则 $\omega^Tx_i+b>0$ 
2. 若 $y_i=-1$ ，则 $\omega^Tx_i+b<0$ 
如果： $y_i=+1$ 或 $-1$ 
一个训练集 $\{(X_i,y_i)\}$，在 $i=1 \sim N$ 线性可分，是指存在 $(\omega,b)$ ，使得对 $i=1 \sim N$ ，有：

$$
y_i(\omega^TX_i+b)>0
$$

<br>

**感知机算法（Perceptron Algorithm）**
1. 随机选择 $W$ 和 $b$
2. 取一个训练样本 $(X,y)$
* 若 $W^TX+b>0$且 $y=-1$，则
    $w=w-x,b=b-1$
* 若 $W^TX+b<0$且 $y=+1$，则
    $w=w+x,b=b+1$
3. 再取另一个 $X,y$，回到2
**终止条件**：直到所有输入输出对都不满足2中两条之一，退出循环。

**感知机算法的优点与缺陷**

感知机算法虽然对比其他模型并没有优势，且不能解决线性不可分问题，但其启发了神经网络的发展。

<br>

**神经网络**
* 在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个**非线性的**函数关系，这个函数称为激活函数。
* 如果层与层之间不加非线性函数，多层神经网络会退化为单神经元感知机。
* 证明：（以最简单两层神经网络为例）
对于网络

$$
a_1=\omega_{11}x_1+\omega_{12}x_2+b_1 \quad 第一个神经元
$$

$$
a_2=\omega_{21}x_1+\omega_{22}x_2+b_2 \quad 第二个神经元
$$

$$
Z_1=\varphi(a_1)    \quad    非线性函数
$$

$$
Z_2=\varphi(a_2)    \quad    非线性函数
$$

$$
y=\omega_1z_1+\omega_2z_2+b_3    \quad    第三个神经元
$$

有

$$
y=\omega_1(\omega_{11}x_1+\omega_{12}x_2+b_1)+\omega_2(\omega_{21}x_1+\omega_{22}x_2+b_2)+b_3
$$

$$
y=(\omega_1\omega_{11}+\omega_2\omega_{21})x_1+(\omega_1\omega_{12}+\omega_2\omega_{22})x_2+\omega_1b_1+\omega_2b_2+b_3
$$

设$W_1,W_2,B$为三个常数

$$
y=W_1x_1+W_2x_2+B
$$

如果层与层之间不加非线性函数，多层神经网络会退化为单神经元感知机

**部分激活函数**

![激活函数](https://pic2.zhimg.com/v2-2762505c22da213c53765fd5a2ccf8c5_1440w.jpg)
**定理**：当 $\varphi(x)$为阶跃函数时，三层神经网络可模拟任意非线性函数

<br>

## 第八章 多层神经网络

<a name="8th"></a>

**设计神经网络的两个问题**
* 网络有多少层
* 每层网络的神经元个数是多少
> 这两个问题共同决定了问题的复杂度与训练样本的数量

<br>

**对于多层神经网络的梯度下降法**

由于多层神经网络中，前一层的输出是后一层的输入，所以函数可用复合函数的形式描述。
对与复合函数，使用梯度下降法计算下次迭代值时，可使用链式求导法则，得到参数的更新值。
例如：

$$
y=f(x) \quad z=g(y)
$$

有

$$
\frac{dz}{dx}=\frac{dz}{dy} \cdot \frac{dy}{dx}
$$


<br>

**反向传播算法**

反向传播算法的梯度求解利用了链式求导法则
反向传播的流程：
1. 前向传播
2. 计算损失
3. 反向传播
4. 更新参数
5. 反复迭代

<br>

**分类器**

分类器是将输入数据（特征向量）通过网络处理，得到预测值（目标向量/数据的标签）
例如：
|输入|标签|实例|
|:-:|:-:|:-:|
|一组相关数据( $x_1,x_2,...,x_n$)|肿瘤类别|肿瘤分类|
|一张图片（ $w\cdot h \cdot 3$大小的向量）|图像类别|图像分类|
|一段文字（字数 $\cdot$编码长的向量）|文本类别|文本分类|

<br>

**分类器的概率函数**

在分类器中，我们选择使用**softmax**函数计算概率，使用**交叉熵损失函数**计算损失
$y=e^x$函数
![softmax函数](https://static.leiphone.com/uploads/new/sns/blogSpe/article/202011/5f9fc44628a99.png)

$$
max(Z_i)=\frac{Z_i}{\Sigma^n_{i=1}Z_i}
$$

$$
softmax(Z_i)=\frac{e^{Z_i}}{\Sigma^n_{i=1}e^{Z_i}}
$$

$$
\Sigma softmax(Z_i)=1
$$

在这里， $n$为节点的个数，即类别的数量， $Z_i$是第 $i$个节点的输出值。

可以看到，使用指数形式的softmax函数能够将差距大的数值距离拉的更大

<br>

**分类器的损失函数**

交叉熵损失（Cross Entropy Loss）函数常被用于计算分类器的损失
![交叉熵损失函数](https://i-blog.csdnimg.cn/blog_migrate/3e3bdbdc0ecea35265555ac08c160949.png)

$$
Loss=\Sigma^n_{i=1}(-y_i log (\hat{y_i}))
$$

其中：
* $n$表示分类的标签数，如果 $n=3$，则表示为三分类问题
* $\Sigma{softmax(Z_i)}=1$，因此 $\Sigma y_i=1$
* $y_i=0或1，Loss=-y_{标签类} \cdot log(\hat{y}\_{标签类})，y_{标签类}=1$ 
* $Loss=-log( \hat{y}_{标签类})$
* $\hat{y}_{标签类} \to 1,Loss \to 0$
* $\hat{y}_{标签类} \to 0,Loss \to +\infty$
* 预测正确时，损失函数无限趋近于0

<br>

**混淆矩阵**

这是用于评估分类器分类性能的一种工具
例如：
||Predicted Positive|Predicted Negtive|
|:-:|:-:|:-:|
|**Actual Positive**|True Positive(TP)|False Negtive(FN)|
|**Actual Negtive**|False Positive(FP)|True Negtive(TN)|
* **True Positive(TP)**:实际为正类，预测为正的样本数
* **False Positive(FP)**：实际为负类，预测为正类的样本数（类型I错误）
* **False Negtive(FN)**：实际为正类，预测为负类的样本数（类型II错误）
* **True Negtive(TN)**：实际为负类，预测为负类的样本数

通过混淆矩阵，我们可以计算出一些评价模型的性能指标，如
* **准确率（Accuracy）**

$$
Accuracy=\frac{TP+TN}{TP+FP+TN+FN}
$$

表示模型预测的正确比例

* **精确率（Precision）**

$$
Precision=\frac{TP}{TP+FP}
$$

预测为正类中，实际为正类的比例，也叫查准率

* **召回率（Recall）**

$$
Recall=\frac{TP}{TP+FN}
$$

实际为正类中，预测为正类的比例，也叫查全率

* **F1值（F1 Score）**

$$
F1 Score=2 \cdot \frac{Precision \cdot Recall}{Precision + recall}
$$

精确率和召回率的调和平均，综合考虑精度和召回度

<br>

**梯度消失**

梯度消失时深度学习中常见的问题，特别是在训练深度神经网络时。它指的是在反向传播过程中，网络的梯度（即损失函数对各层权重的导数）变得非常小，导致权重更新及其看缓慢，甚至完全停止更新，会使底层网络参数不更新，退化为一个浅层的神经网络。
* 梯度消失问题一般发生在网络非常深（即层数很多）时。
* 梯度消失问题会导致网络权重更新缓慢、深层网络难以训练和训练停滞等问题。
* 梯度消失问题的解决方式为：使用合适的激活函数、更改权重初始化的方法、使用批量归一化和使用残差网络等方式。

<br>

**梯度爆炸**

梯度爆炸是与**梯度消失**相对的一个问题，通常出现在深度神经网络的训练过程。梯度爆炸指的是在反向传播时，梯度的数值变得非常大，导致网络的权重更新过大，进而导致模型训练不稳定，甚至无法收敛。
* 梯度爆炸的原因与梯度消失类似，为：激活函数的梯度过大，权重初始化不当、长时间的反向传播等
* 梯度爆炸会导致权重更新不稳定、训练过程无法收敛和数值溢出等问题
* 梯度爆炸的解决方案：梯度裁剪、使用适当的权重初始化方法、使用合适的激活函数、调整学习率、使用批量归一化和使用残差网络等

<br>

**参数和超参数**
* 参数（Parameter）：是**通过训练**得到的参数
* 超参数（Hyper Parameter）：是在开始学习之前人为设置的参数，而**不是**通过训练得到的
> **调参**指的也是调整超参数

<br>

## 第九章 卷积神经网络

<a name="9th"></a>

**回顾**
* **机器学习**：通过**学习算法**，从**数据**中学习到**模型**的过程

<br>

**特征工程**
* 尽可能**选择**和**构建**出好的特征，使得**机器学习算法**能够达到最佳性能。

<br>

**传统机器学习的特点**
* 依赖**人工方式**提取和设计特征
* 需要大量的**专业知识**和**经验**
* 特征设计和**具体任务**密切相关
* 特征的**计算**、**调整**和**测试**需要大量的时间

<br>

**深度神经网络**
* 有**多层隐含层**的神经网络

<br>

**端到端学习**
* **自动**从**数据**中**学习**特征

<br>

**数据驱动**
* 当**某个任务**的**数据量**大到一定程度，机器就可能在**该任务**上超过人类

<br>

**多层神经网络的劣势**
* 数学不漂亮。优化算法智能获得局部极值，算法性能与初始值有关。
* 不可解释。训练神经网络获得的参数与实际任务的关联性非常模糊。
* 如果要训练相对复杂的网络，需要大量的训练样本。
* 模型可调整的参数很多，使得训练神经网络变成了一门“艺术”，被网友戏称为“炼丹”

<br>

**深度学习三要素**

数据、算法和计算力
* **数据量**越大，深度学习的优势越明显
* **大规模深层神经网络**需要**算法创新**和**改进**，使深度学习的**性能**和**速度**得到保障
* 训练**大规模深层神经网络**，需要**强大的计算资源**

<br>

**复杂的全连接网络所面临的问题**

维度灾难，即内存、计算量巨大，模型训练困难
* 问题在于：待估参数的初始化
* 假如初始参数完全随机，如果神经网络网络过浅，找到的局部最小值极有可能是一个很差的点

<br>

**自编码器**

在2006年，Geoffrey Hinton提出了一种叫做**自编码器**（auto-encoder）的方法，它部分地解决了神经网络初始化参数的问题
* 自编码器采用了分层初始化的思想
* 其方法具体为
  - 步骤1：先训练一个小的网络
  - 步骤2：训练好第一层后，接着训练第二层
  - 步骤3：依次类推，训练好n-1层后，训练第n层
  - 步骤4：最后用反向传播对网络进行微调
如此，每一层都包含训练数据的局部信息，大概率找到的局部极值点效果不错。
> 自编码器是一种神经网络的**初始化参数方法**

<br>

**语义鸿沟（Semantic Gap）**

指图像的底层视觉特性和高层语义之间的鸿沟
* 相似的视觉特性，不同的语义概念
* 不相似的视觉特性，相同的语义概念

<br>

**卷积神经网络**
* **卷积神经网络**（Convolutional Neural Networks,CNN）是一类包含卷积计算并且具有深度结构的前馈神经网络。深度学习的代表算法。
* 卷积神经网络仿造生物的**视知觉**（Visual Perception）机制构建，能够进行平移不变分类。
* 对卷积神经网络的研究始于二十世纪80至90年代，**LeNet-5**是较早出现的卷积神经网络。
* 卷积神经网络具有以下特点
    - 局部卷积
    - 参数共享
    - 多卷积核
    - 池化操作
    - 多层处理
![边缘提取卷积核](https://i-blog.csdnimg.cn/blog_migrate/205c37659cab98919bab67f9127eaf6e.jpeg)
一种用于边缘提取的卷积核

<br>

**感受野**

神经元所影响的刺激区域称为神经元的**感受野**（Receptive Field），不同的神经元感受野的大小和性质都不同
在机器学习方面的感受野定义为：
* 卷积神经网络输出特征图上的像素点 在**原始图像上所能看到区域**的大小，输出特征会受感受野区域内的像素点的影响
![感受野图片](https://i-blog.csdnimg.cn/blog_migrate/cc723d5b0c69cdeed2f13e68b4ad2dd4.png)
在这幅图中，输出特征图2的一个像素的感受野为5*5

<br>

**CNN的特点之一：局部卷积**
* 可以把卷积想象成作用于矩阵的一个滑动窗口函数。华东窗口又称作**卷积核**、**滤波器**或是**特征检测器**。
* 对于给定的输入图像，输出特征图中每个像素实际上是输入图像中局部区域中像素的加权平均，其均值由**卷积核**定义。
![局部卷积示意图](https://pica.zhimg.com/v2-f953698750c3a0a75c5a4e0bccf6e5de_1440w.jpg)
卷积示意图

<br>

**卷积的实际操作**

使用卷积核对原始图像的局部窗口做哈达马积，得到特征图的一个像素点，然后将窗口滑动一格（步长为1时，为2则移动两格），然后重复上述操作。
![卷积计算示意图](https://img2020.cnblogs.com/blog/1990595/202012/1990595-20201207113920449-408231101.jpg)
卷积具体计算方式示意图

<br>

**CNN特点之二：参数共享**

在移动滑窗的过程中，卷积核的参数值是固定的，即无论滑窗在什么地方，使用的都是同样的卷积核。

<br>

**CNN特点之三：多卷积核**

为了充分提取特征，可以使用多个卷积核
* 每个卷积核都会对输入图像进行卷积处理，生成一幅**特征图**。不同卷积核生成的不同图像可以理解为是该输入图像的**不同通道**。
![多核卷积](https://img2018.cnblogs.com/blog/1825659/201910/1825659-20191011203637692-218830366.gif)
多核卷积示意图

<br>

**CNN特点之四：池化处理**
* 池化处理也叫作**降采样处理**（down-pooling），是对不同位置的特征进行聚合统计。通常是去对应位置的最大值（最大池化）、平均值（平均池化）等。
* 池化的优点：
    - 降维
    - 克服过拟合
    - 在图像识别领域，池化还能提供平移和旋转不变性
![池化示例](https://i-blog.csdnimg.cn/blog_migrate/7185252467348322d9cf07626cbdd1e3.png)
平均池化和最大池化示意图

<br>

**CNN特点之五：多层处理**
* 一般而言，在图像处理中，一层卷积及降采样往往只学到了**局部的**特性。层数越多，学到的特征越**全局化**。因此通过这样的多层处理，**低级**的特征组合形成**更高级**的特征表示。
![多层处理示意图](https://i-blog.csdnimg.cn/blog_migrate/00318bb1a90bd32487471b22599b5c28.png)
多层处理示意图

<br>

**LeNet-5模型分析**

LeNet-5是现代卷积神经网络的起源之一，由Yann Lecun于1998年提出，一共有七层。输入图像尺寸为**32\*32**，有两个**卷积层**，两个**降采样层**，三个**全连接层**。输出为输入图像的**数字类别（0-9）概率**。
* **卷积层**：C1层有6个卷积核（核大小为5*5），输出6个大小为28\*28的特征图，对应6种局部特征
* **降采样层**：S2是降采样层（Down Sampling），目的是降低网络训练参数及克服模型过拟合。S2层有6个14\*14的feature map，其每个单元于上一层的2\*2领域连接（滑动窗口为2\*2）。所以S2层的size是C1层的1/4。
* **卷积层**：C3卷积层，每个特征图只与上一层S2中部分特征图相连接。为多通道16核卷积，有16个卷积核，其大小为5\*5，输出16个特征图，每个大小为 $(14-5+1)*(14-5+1)=10*10$。
* **降采样层**：S4层有16个5\*5的特征图，其每个单元与上一层的2\*2邻域连接（滑动窗口为2\*2）。所以S4层的size是C3层的1/4。
* **全连接层**：F5将每个大小为为5\*5的特征图拉成一个长度为400的向量，并通过一个带有120个神经元的全连接层进行连接。120是由LeNet-5的设计者根据实验得到的最佳值。
* **全连接层**：F6是全连接层，类似MLP多层神经网络中的一个layer，共有84个神经元。这84个神经元与C5层进行全连接。如经典的神经网络，F6层计算输入向量和权重向量之间点积，再加上一个偏置。然后将其传递给Sigmoid函数进行非线性变换。
* **高斯连接层**：输出层由欧式径向基函数（Euclidean Radial Basis Function）单元构成。每个输出RBF单元计算输入向量和参数之间的欧式距离。

$$
y_i=\Sigma^{84}\_{j=1}(x_j-w_{ij})^2
$$

![LeNet-5网络结构图](https://res-static.hc-cdn.cn/cloudbu-site/china/zh-cn/HMP-PEPContent/images_160611867075480.png)
LeNet网络结构图

<br>

**MINIST**
* Yann Lecun值作了MINIST数据集，用于训练LeNet
* MINIST数据集：
    - 60000个训练样本
    - 10000个测试样本
* LeNet的MINIST识别率：
    - 98.5%
* LeNet提出了图像卷积的概念

<br>

**文本信息的矩阵化处理**

对于无结构的文本信息进行卷积处理，可以假设文本T长度为L，则文本也可以转化为矩阵形式。
* 基于CNN的文本分类网络：TextCNN

<br>

## 第十章 循环神经网络

<a name="10th"></a>

**循环神经网络**

循环神经网络（Recurrent Neural Network,RNN）
是一类以序列（Sequence）数据为输入，在序列的演讲方向进行递归（Recrursion）且所有节点（循环单元）按链式连接的递归神经网络（Recursive neural network）

<br>

**自然语言处理**
* **序列**一般指自然语言的数据。
* **序列数据**是指有先后次序的一组数据。
* **自然语言**（Natural Language）通常是指一种自然地随文化演化的语言。例如，汉语、英语、法语等。
* **不属于**自然语言的例子：C语言、Python语言、鸟语、暗夜精灵语、世界语等。

<br>

**从文本提取特征的两种方法**
* **独热编码**（One Hot）
    - 每个词表示为一个很长的向量。向量的维度是词表的大小，其中只有一个维度的值为1，其他元素为0。
    - 存在“词汇鸿沟”现象，即无法从编码中看出词汇的关系。
* **词向量**（Word Embedding）
    - 将词语映射到一个**低微向量空间**（一般50、100、200或300维），使用一组**低微实数向量**表示一个词，使语义上相似或**相关**的词在距离上更**相近**。
> 判断两词是否有关可以用**余弦相似度**来表示

$$
余弦相似度：cos\theta=\frac{A \cdot B}{|A|*|B|}
$$

<br>

**word2vec**
* word2vec使Google公司在2013年开源的将词表示为低维实数值向量的工具。
* **word2vec**包括：
    - 跳字模型（**skip-gram**）：用当前词来预测上下文
    - 连续词袋模型（**CBOW**，continuous bag of words）：用上下文来预测当前词
* 可以从[这里](http://word2vec.googlecode.com/svn/trunk/)下载代码
例如：利用word2vec进行词的相似度计算
```python=
#./distance vectors.bin
Enter word or sentence(EXIT to break):china
Word:china Position in vocabulary: 486 Word
Cosinedistance
-------------------
taiwan 0.656181
japan 0.633499
tibet 0.607813
hainan 0.561931
xiamen 0.555860
chongqing 0.550099
chinese 0.548320
```

<br>

**RNN的缺陷**
* 前部序列信息在传递到后部的同时，信息权重下降，导致重要信息丢失
* 需要提高前部特定信息的决策权重
* 长距离语义消失

<br>

## 第十一章 自然语言处理

<a name="11st"></a>

**SQuAD**（阅读理解）
* SQuAD是由斯坦福大学发布的一个机器阅读理解数据集。该数据集包含10万个三元组（问题、原文、答案），原文来自于536篇维基百科文章。
* 对于每个文章的问题（<=5），有很多标注人员标注答案，且答案出现在原文中。

<br>

**SNLI**（斯坦福自然语言推断）
* 主要研究假设（hypothesis）是否可以从前提（premise）中推断出来
* 自然语言推断决定了一对文本序列之间的三种逻辑关系：蕴含、矛盾、中性

<br>

**NER**(命名实体识别)
* 命名实体识别（NER）是信息提取的一个子任务，旨在将文本中的命名实体定位并分类为预先定义的类别，如人员、组织、位置、时间表达式、数量、货币值、百分比等。

<br>

**ChatGPT编程**
ChatGPT在**代码生成**中的应用可以通过输入关键词、选择编程语言、生成代码等步骤来实现。具体步骤如下：
* 输入关键词。用户可以根据需求输入关键词，例如，“生成随机数列表”“读取 CSV 文件”等。关键词的准确性和清晰度将对生成的代码质量产生重要的影响。
* 选择编程语言。ChatGPT 支持多种编程语言，例如，Python、Java、JavaScript 等。用户可以根据自己的需求和熟练程度选择合适的编程语言。
* 生成代码。根据用户输入的关键词和选择的编程语言，ChatGPT可以生成对应的代码。用户可以对生成的代码进行调整和优化，以满足自己的需求。

<br>

**时间序列分析**
* **时间序列**是指将同一统计指标的数值按其发生的时间先后顺序排列成的数列。
* 时间序列预测可应用于经济学、气象学、金融学、医学等。
* 某些时间序列预测模型源于自然语言处理模型，如：LSTM、GRU、Transformer

<br>

**长短时记忆神经网络**（LSTM）

具有：
* 遗忘门：将值朝0减少
* 输入门：决定不是忽略掉输入数据
* 输出门：决定是否使用隐状态

<br>

**ELMO**（Embeddings from Language Model）
* 基于LSTM
* 用于下游任务

<br>

**各模型用途**

|模型|用途|
|:-:|:-:|
|SQuAD|阅读理解|
|SNLI|自然语言推理|
|SRL|语义角色标注|
|Coref|指代消解|
|NER|命名实体识别|
|SST-5|情感分析|

<br>

**注意力机制**

有的时候，我们希望模型关注一些重要的**局部信息**，而不是把总体的所有信息一视同仁地对待。
在Attention诞生之前，已经有CNN和LSTM模型了，为什么还要引入Attention机制？
1. **计算能力**的限制：如果要记住很多“信息”，模型就要变得更复杂。
2. **优化算法**的限制：LSTM只能在一定程度上缓解RNN中的长距离依赖问题，且信息“记忆”能力并不高。
3. CNN、LSTM实际上也不能决定哪些信息重要

<br>

**Transformer**
* Transformer是Google的团队2017年提出的一种NLP经典模型
* 目前主流的几种语言模型BERT、GPT都是基于Transformer
* Transformer可用于传统NLP任务、时间序列分析、机器视觉
* Sequence-to-sequence模型
* 序列到序列模型

<br>

**词Embedding**
* 单词的Embedding有很多种方式可以获取，例如可以采用Word2Vec、Glove等算法预训练得到，也可以在Transformer中训练得到

<br>

**位置Embedding**
* Transformer中除了单词的Embedding，还需要使用位置Embedding表示单词出现在句子中的位置。因为**Transformer**不采用**RNN**的结构，而是使用全局信息，不能利用单词的顺序信息，
* 而这部分信息对于**NLP**来说非常重要。所以Transformer中使用位置Embedding保存单词在序列中的相对或绝对位置

> 将单词的词 Embedding 和位置 Embedding 相加，就可以得到单词的表示向量 x，x 就是 Transformer 的输入

<br>

**其他**
* 层归一化（Layer Normalization）的作用是把神经网络中隐藏层归一为标准正态分布，以起到加快训练速度、加速收敛的作用。
* Feed Forward 是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下:

$$
max(x,XW_1+b_1)W_2+b_2
$$

X是输入，Feed Forward 最终得到的输出矩阵的维度与 X 一致。

<br>

**BERT的意义**

是一种基于Transformer架构的预训练语言模型，主要用于自然语言处理（NLP）任务。‌‌
* 从大量无标记数据集中训练得到的深度模型，可以显著提高各项自然语言处理任务的准确率
* 用语言掩码模型(MLM)方法训练词的语义理解能力
* 用下局预测(NSP)方法训练句子之间的理解能力，从而更好地支持下游任务
* BERT参考了ELMO模型的双向编码思想
* 借鉴了GPT用Transformer作为特征提取器的思路
* 采用了word2vec所使用的CBOW方法(连续词袋模型，用上下文预测一个词)
